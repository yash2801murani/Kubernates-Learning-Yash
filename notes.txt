replication controller 

Replicaset - Extendent version of replication controller 


Both are working same but Replicaset is has some more features. 



---------------------------------------------------------------------------------------------------------------------------------------

Que....

Supose we have 3 pod working with nginx without labels and now we want Replicaset to manage this pods and ensure keep on 3
can working continuously. 

so can we need to add template pod in Replicaset yaml?? please note we have already running pods with image and all. 

Ans : Yes.... Definately required bcz once pod get failed Replicaset requires pod template to initiate the new pod. 


---------------------------------------------------------------------------------------------------------------------------

kubectl scale --replicas=6  Replicaset/replicasetname


---------------------------------------------------------------------------------------------------------------------------

Deployments

kubectl set image deployment/<deployment-name> <container-name>=<new-image>

üîπ Update Deployment Image
EG : kubectl set image deployment/nginx-deploy nginx=nginx:1.27

üîπ View Rollout Status
kubectl rollout status deployment/nginx-deploy

üîπ View Rollout History
kubectl rollout history deployment/nginx-deploy


üîπ Rollback to Previous Version
kubectl rollout undo deployment/nginx-deploy

---------------------------------------------------------------------------------------------------------------------------


Services 

‚úÖ What is a Service?

A Service in Kubernetes is a permanent IP and name
that helps other apps connect to your Pods.

Pods keep changing‚Ä¶
Service stays same always.

üü© Why do we need Services?
1Ô∏è‚É£ Pods are not permanent

Pods die, restart, new pods come ‚Üí their IP changes.
Service gives one fixed IP.

2Ô∏è‚É£ To access the application

If you want to reach your app running in pods,
you need a service to expose it.

3Ô∏è‚É£ Load balancing

If you have multiple pods,
Service sends traffic to all of them (round-robin load balancing).

4Ô∏è‚É£ Communication between apps

Backend ‚Üí Frontend
Frontend ‚Üí Database
Services connect them easily.

5Ô∏è‚É£ Stable DNS name

Example:
nginx-service.default.svc.cluster.local

Always same, even if pods change.

üü¶ Types of Services (very simple)
1. ClusterIP (default)

Works inside cluster only

No external access

Used for internal communication
Example: backend communicating with database.

2. NodePort

Opens a port on every node

You can access service using NodeIP:NodePort

Used for testing / exposing to outside world

3. LoadBalancer

Cloud providers (AWS, GCP, Azure)

Creates a real load balancer

Best for production external apps

4. ExternalName

Maps a service name to an external URL
Example: MySQL on another server

üü© In one line:

Pods keep changing. Service gives a stable way to connect to them.


In Depth node port


In all the worker node same port should be open 30000-32767

port : port of svc
nodePort : Port of the actual Machine
Targetport : port of the container where service is running 


if i create node port service for nginx where pods are in 3 multiple worker node 

means 3 worker node ip should open same nodeport port and then using any ip of worker node you can access your resource using the same nodeport number. 




Cluster IP

Internal communication between pods and vm in cluster
not for exposing out side the cluster

eg communicating between frontend and backend. 

allocate single ip called ClusterIP

Your pod knows which service to call because YOU (the developer) give it the service name (like DB_HOST=mysql-service), and Kubernetes DNS + kube-proxy handle the routing internally.


Load Balancer. 

Focus on the frontend pods 

LB Service -> Pod1(Node1)
           -> Pod2(Node2)
           -> Pod3(Node3)

---------------------------------------------------------------------------------------------------------------------------



Namespace :

Logical devide of k8s cluster

Kubernetes Namespace ‚Äî Simple Notes

A namespace is a way to divide a Kubernetes cluster into smaller logical groups.

It works like separate folders inside the same cluster.

Helps in organizing resources and avoiding name conflicts.

Used to separate environments like dev, test, prod.

You can set resource limits per namespace.

You can control who can access what using RBAC for each namespace.

Some namespaces are created by default:
default, kube-system, kube-public, kube-node-lease.

Resources inside one namespace cannot see resources in another unless exposed.

If you don‚Äôt mention a namespace, Kubernetes uses the default one.


---------------------------------------------------------------------------------------------------------------------------

explanation of Imperative vs Declarative ‚Äî based on what you actually do in real clusters üëá

üü¶ Imperative in Kubernetes (Practical Understanding)

You give Kubernetes direct commands to perform actions immediately.

Practical examples:

You want a pod ‚Üí you run a command ‚Üí pod is created.

You want to delete a service ‚Üí you run a command ‚Üí service is removed.

You want replicas = 5 ‚Üí you run a command ‚Üí replica count changes.

ü§î What happens practically?

No history

No YAML

You must remember all commands manually

Not suitable for big apps

Real commands (for understanding, not explanation):

Create pod

Scale deployment

Delete pod

These actions happen instantly when you run the command.

üü© Declarative in Kubernetes (Practical Understanding)

You create a YAML file that describes your final desired state.

Practical examples:

You create a file that says:
‚ÄúThis deployment should always have 3 pods.‚Äù
‚ÄúThis service should expose port 80.‚Äù

Kubernetes continuously maintains that state.

If a pod is deleted, Kubernetes recreates it automatically.

If someone changes something manually, Kubernetes puts it back to the desired state.

ü§î What happens practically?

YAML acts like documentation

Version controlled (Git)

Easy for teams

Kubernetes self-heals according to the YAML

Real usage:

You apply YAML files to deploy apps

You use Git to store these files

You use tools like ArgoCD / Flux for GitOps


---------------------------------------------------------------------------------------------------------------------------

Kubectl apply command

localfile : yaml
liveObjectConfiguration : K8s managerd
JSON file : k8s Managed

Any change made on local file

kubectl apply -f file name make change in the liveObjectConfiguration

then it finally updated into the json file which is managed by k8s it self. 

stored into the liveObjectConfiguration with in annotation with lastappliedconfiguration.json

---------------------------------------------------------------------------------------------------------------------------

 When manual schuduling happens if you want that specific pod should go to specific node
 in deployment file under metadata use nodeName : nodenameofworker to schuduling the pod of required node only. 
 
---------------------------------------------------------------------------------------------------------------------------

Taint and tolarance


üõ°Ô∏è What is a Toleration?

A toleration is given to a pod so that it can ignore the taint and still run on that node.

Think of toleration as:

‚úÖ ‚ÄúSpecial Permission‚Äù badge for the pod.

üéØ Simple Example
Node:
Node-1: has taint ‚Üí No pods allowed.

Pod:
Pod wants to run.
But pod has NO toleration.


üëâ So Kubernetes will NOT schedule the pod on Node-1.

üß† But if pod has toleration:

Add:

tolerations:
- key: "team"
  value: "db"
  effect: "NoSchedule"


Then pod gets permission.

üëâ Now Kubernetes WILL schedule it on Node-1.



Commands:

kubectl taint nodes <node-name> <key>=<value>:<effect>

kubectl taint nodes node1 app=db:NoSchedule
This means:

key = app

value = db

effect = NoSchedule

node = node1

üí° Now normal pods CANNOT run on this node.


containers:
  - name: app
    image: php:8.2
  tolerations:
  - key: "app"
    value: "db"
    effect: "NoSchedule"


---------------------------------------------------------------------------------------------------------------------------

Node Selectors:

1st need to label the node 

suppose i have 3 nodes 

1= large
2= mid 
3 = small

1st node i label as size=large
2nd node i label as size = min
3nd node i label as size = small

then in pod or deployment file in spec we can specify the nodeSelector size=large || size=small as per requirement. 

 
 label command for node

 kubectl label node worker01 size=large


Limitation?

If i have requirement which is complex such as schule pod on node which is not small
or schule on where node is either small or medium

this is not archiveable so that we need node affinity. 


---------------------------------------------------------------------------------------------------------------------------

Node affinity

Provide advance expression for selecting the node for specific pod. 

Advance version of node Selectors

---------------------------------------------------------------------------------------------------------------------------


Taint, Tollarance vs Node affinity. 

IMP. 


---------------------------------------------------------------------------------------------------------------------------



Resource Limitation

CPU 
Memory

Min neeeds to add in deployment. 

cpu can start from 0.1 as 100MB  
1 cpu means 1VCPU.

Memory min to 256 M or Mi or in GB as 1Gi


 we can set limits as well for using max how much pod can use

 limit also set under requests


What if goes above the limits set?

CPU : Throttles can not use more then limit
Memory: container may use above while once go above time then go with OOM out of memory and kill the container


what is best?


FOR. 

No Req no limit
req set  but not limit || This might best but tottaly based on requirement. 
no req but limit set
req and limits  both set 


if we didnt set? in the namespace level it is by default specified for the each pod and the container.



---------------------------------------------------------------------------------------------------------------------------
IMP command. 

Create YAML from the pod

kubectl get pod elephant -o yaml > elephant-pod.yaml


---------------------------------------------------------------------------------------------------------------------------


Daemonset

Is all about create one pod and container on each worker node. it is the usecase of the Daemonset

---------------------------------------------------------------------------------------------------------------------------

Static pod

Work only for pod not for replicaset and all. 

Suppose there is no master node noKubeapi server how kubelet on worker node can create pod?

well kubelet can do so how?

there is def fdirectory when we install kubelet is etc/kubernates/menifest 

so we can create yaml file for pod in this dir and kubelet can run that pod on that worker node (only pod no deplotment or rs)

Please learn use case of this ......?

---------------------------------------------------------------------------------------------------------------------------


Priority classes. 


EG 

k8s components like apiserver etcd are the highest Priority
then might db pod is high Priority
then might other comes into the picture. 
other job might have lowest priority compare to k8s components. 


Set priority in range 

-2B to 1B is the value. (For apps on cluster)

Priority between 1B to 2B is for k8s component like apiserver etcd cluster or master node component. 

 k get priorityclass

 need to create priorityclass using yaml file first 
 and then we can use proporty in pod deployment file as priorityClassName: under the container: component. 


 in pod default priority value is 0

  
needs to understand suppose we have job or pod of priority with 1 and resources are full and then comes another job or pod
with higherpriority supose 10 what about running lower priority job?


its based on preamtionPolicy default it is set to preeemtLowerPriority means kill the lower priority job and assign resourcers
to higher priority new jobs. 

 we can set to never so it keeps run lower pirotity jobs it get executed first. 


---------------------------------------------------------------------------------------------------------------------------

