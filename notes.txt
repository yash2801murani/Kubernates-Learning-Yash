replication controller 

Replicaset - Extendent version of replication controller 


Both are working same but Replicaset is has some more features. 



---------------------------------------------------------------------------------------------------------------------------------------

Que....

Supose we have 3 pod working with nginx without labels and now we want Replicaset to manage this pods and ensure keep on 3
can working continuously. 

so can we need to add template pod in Replicaset yaml?? please note we have already running pods with image and all. 

Ans : Yes.... Definately required bcz once pod get failed Replicaset requires pod template to initiate the new pod. 


---------------------------------------------------------------------------------------------------------------------------

kubectl scale --replicas=6  Replicaset/replicasetname


---------------------------------------------------------------------------------------------------------------------------

Deployments

kubectl set image deployment/<deployment-name> <container-name>=<new-image>

üîπ Update Deployment Image
EG : kubectl set image deployment/nginx-deploy nginx=nginx:1.27

üîπ View Rollout Status
kubectl rollout status deployment/nginx-deploy

üîπ View Rollout History
kubectl rollout history deployment/nginx-deploy


üîπ Rollback to Previous Version
kubectl rollout undo deployment/nginx-deploy


Deployment stratagy types:

1) Destroy old and create new -> Appln downtime -> called Recreate stratagy. 
2) One by one container down and created new -> No down time -> Rolling update


---------------------------------------------------------------------------------------------------------------------------


Services 

‚úÖ What is a Service?

A Service in Kubernetes is a permanent IP and name
that helps other apps connect to your Pods.

Pods keep changing‚Ä¶
Service stays same always.

üü© Why do we need Services?
1Ô∏è‚É£ Pods are not permanent

Pods die, restart, new pods come ‚Üí their IP changes.
Service gives one fixed IP.

2Ô∏è‚É£ To access the application

If you want to reach your app running in pods,
you need a service to expose it.

3Ô∏è‚É£ Load balancing

If you have multiple pods,
Service sends traffic to all of them (round-robin load balancing).

4Ô∏è‚É£ Communication between apps

Backend ‚Üí Frontend
Frontend ‚Üí Database
Services connect them easily.

5Ô∏è‚É£ Stable DNS name

Example:
nginx-service.default.svc.cluster.local

Always same, even if pods change.

üü¶ Types of Services (very simple)
1. ClusterIP (default)

Works inside cluster only

No external access

Used for internal communication
Example: backend communicating with database.

2. NodePort

Opens a port on every node

You can access service using NodeIP:NodePort

Used for testing / exposing to outside world

3. LoadBalancer

Cloud providers (AWS, GCP, Azure)

Creates a real load balancer

Best for production external apps

4. ExternalName

Maps a service name to an external URL
Example: MySQL on another server

üü© In one line:

Pods keep changing. Service gives a stable way to connect to them.


In Depth node port


In all the worker node same port should be open 30000-32767

port : port of svc
nodePort : Port of the actual Machine
Targetport : port of the container where service is running 


if i create node port service for nginx where pods are in 3 multiple worker node 

means 3 worker node ip should open same nodeport port and then using any ip of worker node you can access your resource using the same nodeport number. 




Cluster IP

Internal communication between pods and vm in cluster
not for exposing out side the cluster

eg communicating between frontend and backend. 

allocate single ip called ClusterIP

Your pod knows which service to call because YOU (the developer) give it the service name (like DB_HOST=mysql-service), and Kubernetes DNS + kube-proxy handle the routing internally.


Load Balancer. 

Focus on the frontend pods 

LB Service -> Pod1(Node1)
           -> Pod2(Node2)
           -> Pod3(Node3)

---------------------------------------------------------------------------------------------------------------------------



Namespace :

Logical devide of k8s cluster

Kubernetes Namespace ‚Äî Simple Notes

A namespace is a way to divide a Kubernetes cluster into smaller logical groups.

It works like separate folders inside the same cluster.

Helps in organizing resources and avoiding name conflicts.

Used to separate environments like dev, test, prod.

You can set resource limits per namespace.

You can control who can access what using RBAC for each namespace.

Some namespaces are created by default:
default, kube-system, kube-public, kube-node-lease.

Resources inside one namespace cannot see resources in another unless exposed.

If you don‚Äôt mention a namespace, Kubernetes uses the default one.


---------------------------------------------------------------------------------------------------------------------------

explanation of Imperative vs Declarative ‚Äî based on what you actually do in real clusters üëá

üü¶ Imperative in Kubernetes (Practical Understanding)

You give Kubernetes direct commands to perform actions immediately.

Practical examples:

You want a pod ‚Üí you run a command ‚Üí pod is created.

You want to delete a service ‚Üí you run a command ‚Üí service is removed.

You want replicas = 5 ‚Üí you run a command ‚Üí replica count changes.

ü§î What happens practically?

No history

No YAML

You must remember all commands manually

Not suitable for big apps

Real commands (for understanding, not explanation):

Create pod

Scale deployment

Delete pod

These actions happen instantly when you run the command.

üü© Declarative in Kubernetes (Practical Understanding)

You create a YAML file that describes your final desired state.

Practical examples:

You create a file that says:
‚ÄúThis deployment should always have 3 pods.‚Äù
‚ÄúThis service should expose port 80.‚Äù

Kubernetes continuously maintains that state.

If a pod is deleted, Kubernetes recreates it automatically.

If someone changes something manually, Kubernetes puts it back to the desired state.

ü§î What happens practically?

YAML acts like documentation

Version controlled (Git)

Easy for teams

Kubernetes self-heals according to the YAML

Real usage:

You apply YAML files to deploy apps

You use Git to store these files

You use tools like ArgoCD / Flux for GitOps


---------------------------------------------------------------------------------------------------------------------------

Kubectl apply command

localfile : yaml
liveObjectConfiguration : K8s managerd
JSON file : k8s Managed

Any change made on local file

kubectl apply -f file name make change in the liveObjectConfiguration

then it finally updated into the json file which is managed by k8s it self. 

stored into the liveObjectConfiguration with in annotation with lastappliedconfiguration.json

---------------------------------------------------------------------------------------------------------------------------

 When manual schuduling happens if you want that specific pod should go to specific node
 in deployment file under metadata use nodeName : nodenameofworker to schuduling the pod of required node only. 
 
---------------------------------------------------------------------------------------------------------------------------

Taint and tolarance


üõ°Ô∏è What is a Toleration?

A toleration is given to a pod so that it can ignore the taint and still run on that node.

Think of toleration as:

‚úÖ ‚ÄúSpecial Permission‚Äù badge for the pod.

üéØ Simple Example
Node:
Node-1: has taint ‚Üí No pods allowed.

Pod:
Pod wants to run.
But pod has NO toleration.


üëâ So Kubernetes will NOT schedule the pod on Node-1.

üß† But if pod has toleration:

Add:

tolerations:
- key: "team"
  value: "db"
  effect: "NoSchedule"


Then pod gets permission.

üëâ Now Kubernetes WILL schedule it on Node-1.



Commands:

kubectl taint nodes <node-name> <key>=<value>:<effect>

kubectl taint nodes node1 app=db:NoSchedule
This means:

key = app

value = db

effect = NoSchedule

node = node1

üí° Now normal pods CANNOT run on this node.


containers:
  - name: app
    image: php:8.2
  tolerations:
  - key: "app"
    value: "db"
    effect: "NoSchedule"


---------------------------------------------------------------------------------------------------------------------------

Node Selectors:

1st need to label the node 

suppose i have 3 nodes 

1= large
2= mid 
3 = small

1st node i label as size=large
2nd node i label as size = min
3nd node i label as size = small

then in pod or deployment file in spec we can specify the nodeSelector size=large || size=small as per requirement. 

 
 label command for node

 kubectl label node worker01 size=large


Limitation?

If i have requirement which is complex such as schule pod on node which is not small
or schule on where node is either small or medium

this is not archiveable so that we need node affinity. 


---------------------------------------------------------------------------------------------------------------------------

Node affinity

Provide advance expression for selecting the node for specific pod. 

Advance version of node Selectors

---------------------------------------------------------------------------------------------------------------------------


Taint, Tollarance vs Node affinity. 

IMP. 


---------------------------------------------------------------------------------------------------------------------------



Resource Limitation

CPU 
Memory

Min neeeds to add in deployment. 

cpu can start from 0.1 as 100MB  
1 cpu means 1VCPU.

Memory min to 256 M or Mi or in GB as 1Gi


 we can set limits as well for using max how much pod can use

 limit also set under requests


What if goes above the limits set?

CPU : Throttles can not use more then limit
Memory: container may use above while once go above time then go with OOM out of memory and kill the container


what is best?


FOR. 

No Req no limit
req set  but not limit || This might best but tottaly based on requirement. 
no req but limit set
req and limits  both set 


if we didnt set? in the namespace level it is by default specified for the each pod and the container.



---------------------------------------------------------------------------------------------------------------------------
IMP command. 

Create YAML from the pod

kubectl get pod elephant -o yaml > elephant-pod.yaml


---------------------------------------------------------------------------------------------------------------------------


Daemonset

Is all about create one pod and container on each worker node. it is the usecase of the Daemonset

---------------------------------------------------------------------------------------------------------------------------

Static pod

Work only for pod not for replicaset and all. 

Suppose there is no master node noKubeapi server how kubelet on worker node can create pod?

well kubelet can do so how?

there is def fdirectory when we install kubelet is etc/kubernates/menifest 

so we can create yaml file for pod in this dir and kubelet can run that pod on that worker node (only pod no deplotment or rs)

Please learn use case of this ......?

---------------------------------------------------------------------------------------------------------------------------


Priority classes. 


EG 

k8s components like apiserver etcd are the highest Priority
then might db pod is high Priority
then might other comes into the picture. 
other job might have lowest priority compare to k8s components. 


Set priority in range 

-2B to 1B is the value. (For apps on cluster)

Priority between 1B to 2B is for k8s component like apiserver etcd cluster or master node component. 

 k get priorityclass

 need to create priorityclass using yaml file first 
 and then we can use proporty in pod deployment file as priorityClassName: under the container: component. 


 in pod default priority value is 0

  
needs to understand suppose we have job or pod of priority with 1 and resources are full and then comes another job or pod
with higherpriority supose 10 what about running lower priority job?


its based on preamtionPolicy default it is set to preeemtLowerPriority means kill the lower priority job and assign resourcers
to higher priority new jobs. 

 we can set to never so it keeps run lower pirotity jobs it get executed first. 


---------------------------------------------------------------------------------------------------------------------------

Admission Controllers. 

---------------------------------------------------------------------------------------------------------------------------

Monitor cluster of the kubernates. 

what to monitor?

Pod level monitor. 

Node level monitor.

no buildIn fullfledge monitoring feature by the k8s. 


1 matrix server as per cluster. 

Does not store into the disk means no history data is present if use the matrix server. 

KUBELET ON each worker node has sub component called cAdviser who gets the performance data of the pod and send to matrix server. 


get maytrix server yaml from github and run the matrix server pod in able to see the data. 

Application logs management?

---------------------------------------------------------------------------------------------------------------------------


Application lifecycle management. 

Rolling back and rollBacks. | covered above


---------------------------------------------------------------------------------------------------------------------------

environment variable in k8s. 

use env propery in array form. 

under ports:

Environment Variables in Kubernetes (Simple Notes)
What is an Environment Variable in K8s?

A small piece of information you give to your Pod/Container.

The application inside the container can read it while running.

Example: database name, mode=production, API URL, etc.

They help you avoid writing values directly inside your application code.

How to Set Environment Variables?

You define them inside your Pod/Deployment YAML under the container.

They can come from:

Direct key-value (simple variables)

ConfigMap

Secret

‚úÖ ConfigMap (Simple Notes)
What is ConfigMap?

A Kubernetes object used to store non-sensitive data.

Example:

app name

log level (debug/info)

URLs

any plain text config

Why use ConfigMap?

To keep configuration outside the container image.

You can update ConfigMap without changing the application code.

‚úÖ Secret (Simple Notes)
What is Secret?

Same as ConfigMap but used for sensitive or confidential data.

Example:

passwords

API keys

database credentials

tokens

Why use Secret?

Values are stored in base64 encoded format.

Prevents directly exposing sensitive data inside YAML files.


apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_MODE: "production"
  APP_VERSION: "1.0"

---
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
type: Opaque
data:
  DB_PASSWORD: cGFzc3dvcmQ=   # "password" base64

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
        - name: demo-container
          image: nginx

          env:
            # 1Ô∏è‚É£ Simple env variable
            - name: APP_NAME
              value: "MyDemoApp"

            # 2Ô∏è‚É£ From ConfigMap
            - name: APP_MODE
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: APP_MODE

            # 3Ô∏è‚É£ From Secret
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: app-secret
                  key: DB_PASSWORD




---------------------------------------------------------------------------------------------------------------------------


Multi container pod


‚úÖ 1. Multi-Container Pod (Simple Explanation)
What it is

A Pod in Kubernetes that has more than one container running inside it.

Simple Meaning

One Pod = one mini-computer
Inside it ‚Üí multiple containers working together.

Why we use it

Because sometimes one application needs helper containers.

Example:

One container runs your main app

Another collects logs

Another updates files

Benefits (Interview Points)

They share the same network (same IP, same port space) ‚Üí fast communication

They share storage (volumes)

Helps in creating modular design (each container does a small job)

Reduces overhead of running separate Pods

Disadvantages

If one container crashes, Pod may restart

Harder to scale individually (you cannot scale one container alone)

More complex debugging

‚úÖ 2. Co-Located Containers
What it means (simple)

Containers inside the same Pod that work together.

Simplest Example

Container A ‚Üí main application

Container B ‚Üí small helper process
Both live in the same Pod ‚Üí co-located.

Why use them

They must share files

They must communicate at localhost

They depend on each other tightly

Interview Point

‚ÄúCo-located containers means containers sharing the same Pod, which gives them shared network and storage.‚Äù

‚úÖ 3. Init Container (Simple + Interview)
What it is

A special container that runs first and completes its task, then only main containers start.

Easy Meaning

It ‚Äúinitializes‚Äù or ‚Äúprepares‚Äù the environment.

Real Example

Before starting your app, you need to:

Download configuration

Check DB connectivity

Create necessary folders

This work is done by init container.

Benefits

Ensures app starts only after requirements are ready

Fail-safe: if init fails ‚Üí pod doesn‚Äôt start

Perfect for setup tasks

Disadvantages

Slower Pod start, because init must finish

Init cannot run alongside main containers (only sequential)

Interview Point

‚ÄúInit containers run sequentially and prepare the Pod environment before the main containers start.‚Äù

‚úÖ 4. Sidecar Container (Simple + Interview)
What it is

A helper container that runs along with the main container to provide extra functionality.

Simple Meaning

Main app + helper app running together inside the same Pod.

Everyday Example

Main container ‚Üí your application
Sidecar ‚Üí container that collects logs
Sidecar ‚Üí updates content
Sidecar ‚Üí proxy container (istio/envoy)

Benefits

Adds extra features without modifying main container

Makes architecture cleaner

Reusable helper logic

Disadvantages

Uses extra CPU/RAM

Can cause Pod restart if sidecar fails

Slightly more complex YAML

Interview Point

‚ÄúA sidecar container works alongside the main container providing supportive functions such as logging, syncing, or proxying.‚Äù

‚≠ê One Simple Combined Example (Easy to Understand)

Imagine you have a Pod for a Website:

Main container

Runs the website

Init container

Downloads website content before main app starts

Sidecar container

Continuously collects logs and sends to monitoring system

This is a multi-container Pod with:

One main container

One init container

One sidecar container

All are co-located because they are inside the same Pod.

üî• Interview Questions You Can Expect
1. What is a multi-container Pod?

A Pod with more than one container that work together using shared network and storage.

2. Why use multi-container Pods?

To split responsibilities and use helper containers like logging, monitoring, file sync, etc.

3. What is an init container?

A container that runs before app containers to prepare the environment.

4. Why do we need init containers?

To ensure the Pod only starts when the system is ready (config files, DB check, etc.).

5. What is a sidecar container?

A helper container running alongside the main container to add extra features.

6. Difference between init and sidecar?

Init ‚Üí runs once before main app

Sidecar ‚Üí runs continuously with main app

7. Why are containers called co-located?

Because they live in the same Pod and share network & storage.


Co located VS side Container

in co located no option to select which container run first however in side car the sidecar container we have start side container before main


---------------------------------------------------------------------------------------------------------------------------


AutoScaline

Vertical scaline VS horizontal scaline

Add resources VS Adding more servers. 


horizontal Scale

Kubectl create autoscale deployment nginx --cpu-percentage=60 --min=1 --max=10


Vertical scale

Need to write YAML no itractive way. 

---------------------------------------------------------------------------------------------------------------------------


Inplace pod resizing. 

---------------------------------------------------------------------------------------------------------------------------


Drain, uncorden and corden command 

‚úÖ 1. Cordon

Simple meaning:
üëâ Stop new pods from going to this node.

Existing pods keep running.

Command:

kubectl cordon worker-node1


Example situation:
You want to do maintenance on worker-node1, so you don‚Äôt want new pods to go there.

‚úÖ 2. Uncordon

Simple meaning:
üëâ Allow new pods to get scheduled on this node again.

Command:

kubectl uncordon worker-node1


Example situation:
Maintenance is done; now pods can be scheduled again.

‚úÖ 3. Drain

Simple meaning:
üëâ Move all pods out of the node safely
and
üëâ Stop new pods from coming to this node.

Drain = evict pods + cordon the node

Command:

kubectl drain worker-node1 --ignore-daemonsets --force --delete-emptydir-data


Example situation:
You want to reboot the node or upgrade OS ‚Üí
so you drain the node to empty it.

‚≠ê Super Simple Summary

cordon ‚Üí stop new pods

uncordon ‚Üí allow new pods

drain ‚Üí remove all pods + stop new pods



---------------------------------------------------------------------------------------------------------------------------
